{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad72037",
   "metadata": {},
   "source": [
    "# üåø LeafSense - Plant Disease Detection\n",
    "## ResNet50 Transfer Learning on Kaggle\n",
    "\n",
    "**Goal**: Train a plant disease classifier (38 classes, ‚â•95% accuracy, ‚â§90 min)\n",
    "\n",
    "**Kaggle Setup** (REQUIRED):\n",
    "1. **GPU**: Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "2. **Dataset**: Add Data ‚Üí Search **\"New Plant Diseases Dataset\"** ‚Üí `vipoooool/new-plant-diseases-dataset`\n",
    "3. **Internet**: ON (for packages)\n",
    "\n",
    "**Outputs**: `LeafSense_ResNet50.h5`, `LeafSense_Model.tflite`, `class_indices.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864041f1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# GPU Configuration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # Enable mixed precision for faster training\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"‚úÖ GPU: {len(gpus)} device(s) | Mixed Precision: ON\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU! Enable GPU in Settings ‚Üí Accelerator ‚Üí GPU T4\")\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__} | Keras: {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7b9fc",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Dataset & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c605488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'BATCH_SIZE': 32,\n",
    "    'PHASE1_EPOCHS': 10,  # Frozen base\n",
    "    'PHASE2_EPOCHS': 10,  # Fine-tuning\n",
    "    'INITIAL_LR': 1e-3,\n",
    "    'FINETUNE_LR': 1e-5,\n",
    "    'DROPOUT': 0.3,\n",
    "    'DENSE_UNITS': 512,\n",
    "}\n",
    "\n",
    "# Auto-detect environment (Kaggle vs Local)\n",
    "if Path('/kaggle/input').exists():\n",
    "    # Running on Kaggle - check which dataset is added\n",
    "    kaggle_base = Path('/kaggle/input')\n",
    "    \n",
    "    # Try vipoooool dataset first (has train/valid split)\n",
    "    if (kaggle_base / 'new-plant-diseases-dataset').exists():\n",
    "        BASE_PATH = kaggle_base / 'new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)'\n",
    "        OUTPUT_DIR = Path('/kaggle/working')\n",
    "        print(\"üåê Environment: Kaggle (vipoooool dataset)\")\n",
    "    # Try abdallahalidev dataset (single color folder)\n",
    "    elif (kaggle_base / 'plantvillage-dataset').exists():\n",
    "        BASE_PATH = kaggle_base / 'plantvillage-dataset'\n",
    "        OUTPUT_DIR = Path('/kaggle/working')\n",
    "        print(\"üåê Environment: Kaggle (abdallahalidev dataset - no train/valid split)\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"‚ùå No PlantVillage dataset found!\\n\"\n",
    "            \"Add either:\\n\"\n",
    "            \"  - vipoooool/new-plant-diseases-dataset OR\\n\"\n",
    "            \"  - abdallahalidev/plantvillage-dataset\"\n",
    "        )\n",
    "else:\n",
    "    # Running locally - download dataset using kagglehub\n",
    "    print(\"üíª Environment: Local\")\n",
    "    import kagglehub\n",
    "    \n",
    "    OUTPUT_DIR = Path('./output')\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Download dataset (cached after first download)\n",
    "    print(\"üì• Downloading PlantVillage dataset...\")\n",
    "    dataset_path = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
    "    print(f\"‚úÖ Dataset downloaded to: {dataset_path}\")\n",
    "    \n",
    "    BASE_PATH = Path(dataset_path)\n",
    "\n",
    "# Auto-detect folder structure\n",
    "TRAIN_DIR = BASE_PATH / 'train'\n",
    "VAL_DIR = BASE_PATH / 'valid'\n",
    "\n",
    "# If train/valid don't exist, use 'color' folder (abdallahalidev dataset)\n",
    "if not TRAIN_DIR.exists():\n",
    "    COLOR_DIR = BASE_PATH / 'color'\n",
    "    if COLOR_DIR.exists():\n",
    "        print(\"‚ö†Ô∏è  No train/valid split found. Using 'color' folder.\")\n",
    "        print(\"   Will create 80/20 split from color folder...\")\n",
    "        # Use color folder as single source - we'll split it later in ImageDataGenerator\n",
    "        TRAIN_DIR = COLOR_DIR\n",
    "        VAL_DIR = COLOR_DIR  # Will use validation_split parameter\n",
    "        USE_SPLIT = True\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Dataset structure not recognized!\\n\"\n",
    "            f\"Expected: train/ and valid/ folders OR color/ folder\\n\"\n",
    "            f\"Found in {BASE_PATH}: {list(BASE_PATH.iterdir()) if BASE_PATH.exists() else 'Path does not exist'}\"\n",
    "        )\n",
    "else:\n",
    "    USE_SPLIT = False\n",
    "\n",
    "print(f\"‚úÖ Dataset: {BASE_PATH}\")\n",
    "print(f\"‚úÖ Train: {TRAIN_DIR}\")\n",
    "print(f\"‚úÖ Validation: {VAL_DIR}\")\n",
    "print(f\"‚úÖ Output: {OUTPUT_DIR}\")\n",
    "print(f\"üìä Config: {CONFIG['BATCH_SIZE']} batch | {CONFIG['PHASE1_EPOCHS']+CONFIG['PHASE2_EPOCHS']} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cfff18",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR, target_size=CONFIG['IMG_SIZE'], batch_size=CONFIG['BATCH_SIZE'],\n",
    "    class_mode='categorical', shuffle=True, seed=SEED\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    VAL_DIR, target_size=CONFIG['IMG_SIZE'], batch_size=CONFIG['BATCH_SIZE'],\n",
    "    class_mode='categorical', shuffle=False\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(train_gen.class_indices)\n",
    "\n",
    "# Save class indices\n",
    "with open(OUTPUT_DIR / 'class_indices.json', 'w') as f:\n",
    "    json.dump(train_gen.class_indices, f, indent=2)\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "class_weights = dict(enumerate(compute_class_weight(\n",
    "    'balanced', classes=np.unique(train_gen.classes), y=train_gen.classes\n",
    ")))\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {NUM_CLASSES} classes | {train_gen.samples:,} train | {val_gen.samples:,} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1c410",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Build ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc98f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with ResNet50 base\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze initially\n",
    "\n",
    "# Custom classification head\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(CONFIG['DENSE_UNITS'], activation='relu')(x)\n",
    "x = layers.Dropout(CONFIG['DROPOUT'])(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "model = Model(inputs, outputs, name='LeafSense_ResNet50')\n",
    "\n",
    "print(f\"‚úÖ Model: {model.count_params():,} params | Base frozen: {not base_model.trainable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420a5c6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Phase 1: Train with Frozen Base (10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=Adam(CONFIG['INITIAL_LR']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(OUTPUT_DIR / 'best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"\ude80 Phase 1: Training with frozen base ({CONFIG['PHASE1_EPOCHS']} epochs)...\")\n",
    "\n",
    "# Train Phase 1\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    epochs=CONFIG['PHASE1_EPOCHS'],\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Phase 1 complete: Val Acc = {max(history1.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09435fb2",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Phase 2: Fine-Tune (Unfreeze last 20 layers, 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze last 20 layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(CONFIG['FINETUNE_LR']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Phase 2: Fine-tuning ({CONFIG['PHASE2_EPOCHS']} epochs, unfrozen=20 layers)...\")\n",
    "\n",
    "# Train Phase 2\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    epochs=CONFIG['PHASE2_EPOCHS'],\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Phase 2 complete: Val Acc = {max(history2.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c9b28",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"üìä Final Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"üìä Final Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447eab9",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Models (Keras .h5 + TensorFlow Lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Keras .h5 model\n",
    "h5_path = OUTPUT_DIR / 'LeafSense_ResNet50.h5'\n",
    "model.save(str(h5_path))\n",
    "h5_size_mb = h5_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"‚úÖ Keras Model saved: {h5_path.name} ({h5_size_mb:.2f} MB)\")\n",
    "\n",
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = OUTPUT_DIR / 'LeafSense_Model.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "tflite_size_mb = tflite_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"‚úÖ TFLite Model saved: {tflite_path.name} ({tflite_size_mb:.2f} MB)\")\n",
    "print(f\"\\nüì• Download from Kaggle Output tab ‚Üí Both models + class_indices.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f276777",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ How to Use Models Locally\n",
    "\n",
    "### Load Keras Model (.h5)\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('LeafSense_ResNet50.h5')\n",
    "\n",
    "# Load class mapping\n",
    "with open('class_indices.json', 'r') as f:\n",
    "    class_indices = json.load(f)\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "# Predict\n",
    "def predict_disease(image_path):\n",
    "    img = Image.open(image_path).convert('RGB').resize((224, 224))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_idx = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_idx]\n",
    "    disease_name = idx_to_class[predicted_idx]\n",
    "    \n",
    "    return disease_name, confidence\n",
    "\n",
    "# Example\n",
    "disease, conf = predict_disease('leaf_image.jpg')\n",
    "print(f\"Disease: {disease} ({conf*100:.2f}%)\")\n",
    "```\n",
    "\n",
    "### Load TFLite Model (.tflite) - Faster!\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite\n",
    "interpreter = tf.lite.Interpreter(model_path='LeafSense_Model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Predict\n",
    "def predict_tflite(img_array):\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "    interpreter.invoke()\n",
    "    return interpreter.get_tensor(output_details[0]['index'])\n",
    "```\n",
    "\n",
    "**‚úÖ Both models work on ANY machine with TensorFlow installed!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
